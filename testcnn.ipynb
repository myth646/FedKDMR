{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ebe51e2-1877-4fa9-93d2-1fed4ea7b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "客户端层数分配: [2, 4, 6, 4, 6, 4, 4, 4, 4, 4]\n",
      "分布比例: [0.19514094537368495, 0.7499062374706622, 0.05495281715565283]\n",
      "dict_keys(['features.0.weight', 'features.0.bias', 'features.2.weight', 'features.2.bias', 'features.5.weight', 'features.5.bias', 'features.7.weight', 'features.7.bias', 'features.10.weight', 'features.10.bias', 'features.12.weight', 'features.12.bias'])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "# from models.Nets import *\n",
    "import argparse\n",
    "\n",
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_layers, input_channels=3, base_channels=32, num_classes=10):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.features = self._make_conv_layers(num_layers, input_channels, base_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels * (2 ** ((num_layers - 1) // 2)), num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_conv_layers(self, num_layers, in_channels, base_channels):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            out_channels = base_channels * (2 ** (i // 2))  \n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if i % 2 == 1:  \n",
    "                layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def state_dict(self, exclude_classifier=True):\n",
    "        original_state = super().state_dict()\n",
    "        if exclude_classifier:\n",
    "            \n",
    "            return {k: v for k, v in original_state.items() if not k.startswith('classifier')}\n",
    "        return original_state\n",
    "    \n",
    "    \n",
    "    def get_conv_layers(self):\n",
    "        return [m for m in self.features if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "def generate_client_layer_config(num_level, k, beta=1.0, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    num_types = len(num_level)\n",
    "    alpha = [beta] * num_types\n",
    "    proportion = np.random.dirichlet(alpha)\n",
    "\n",
    "    counts = np.floor(proportion * k).astype(int)\n",
    "    remainder = k - np.sum(counts)\n",
    "\n",
    "    for i in np.random.choice(num_types, remainder, replace=True):\n",
    "        counts[i] += 1\n",
    "\n",
    "    client_layers = []\n",
    "    for count, layer in zip(counts, num_level):\n",
    "        client_layers.extend([layer] * count)\n",
    "\n",
    "    np.random.shuffle(client_layers)\n",
    "\n",
    "    return client_layers, proportion.tolist()\n",
    "    \n",
    "def compare_models_aggregatable_layers(m1, m2):\n",
    "    layers1 = m1.get_conv_layers()\n",
    "    layers2 = m2.get_conv_layers()\n",
    "    min_len = min(len(layers1), len(layers2))\n",
    "    count = 0\n",
    "    for i in range(min_len):\n",
    "        if layers1[i].weight.shape == layers2[i].weight.shape:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def get_shared_keys(models):\n",
    "    from collections import Counter\n",
    "    keys_by_model = [set(m.state_dict().keys()) for m in models]\n",
    "    print( keys_by_model)\n",
    "    shared = set.intersection(*keys_by_model)\n",
    "    conv_keys = [k for k in shared if 'classifier' not in k]\n",
    "    return conv_keys\n",
    "\n",
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(800, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        result = {}\n",
    "        result['activation'] = x\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        result['hint'] = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        result['representation'] = x\n",
    "        x = self.fc2(x)\n",
    "        result['output'] = x\n",
    "        return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_level = [2, 4, 6]\n",
    "    k = 10\n",
    "    beta = 0.7\n",
    "\n",
    "    num_layers_list, prop = generate_client_layer_config(num_level, k, beta, seed=42)\n",
    "    \n",
    "    # num_layers_list=[2,2,4,10,6,4,4]\n",
    "#     trans_cifar10_train = transforms.Compose([transforms.ToTensor(),\n",
    "#                                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "#     # datasets.CIFAR10('./data/cifar10', train=True, download=True, transform=trans_cifar10_train)\n",
    "#     train_set =datasets. CIFAR10('./data/cifar10', train=True, download=True, transform=trans_cifar10_train)\n",
    "#     train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "#     \n",
    "    # model1 = DynamicCNN(num_layers=8, input_channels=3)\n",
    "    # x, y = next(iter(train_loader))\n",
    "    # output = model1(x)\n",
    "    # print(output.shape)\n",
    "    models = [DynamicCNN(n) for n in num_layers_list]\n",
    "    print(models[2].state_dict().keys())\n",
    "    # models = [CNNMnist() for i in num_layers_list]\n",
    "    # print(models[0].state_dict().keys())\n",
    "    layer_groups = defaultdict(list)\n",
    "    for i, model in enumerate(models):\n",
    "        layer_groups[len(model.state_dict().keys())].append(i)\n",
    "    layer_types = sorted(layer_groups.keys()) \n",
    "    \n",
    "#     for i, model in enumerate(models):\n",
    "#         num_conv_params = 2 * len(model.get_conv_layers())  \n",
    "#         layer_groups[num_conv_params].append(i)\n",
    "#     layer_types = sorted(layer_groups.keys()) \n",
    "#     print(layer_types)\n",
    "#     print(layer_groups)\n",
    "    max_layer = max(layer_types)\n",
    "    # w_avg = copy.deepcopy(next(m for m in models if  2 * len(m.get_conv_layers()) == max_layer))\n",
    "    w_avg = copy.deepcopy(next(m for m in models if len(m.state_dict())== max_layer))\n",
    "    print( len(w_avg.state_dict()))\n",
    "#     # conv_layers_list = get_shared_keys(models) \n",
    "#     print(len(conv_layers_list))\n",
    "#     for i, model in enumerate(models):\n",
    "#         layer_groups[len(model.keys())].append(i)\n",
    "#     print(layer_groups)\n",
    "    \n",
    "    # state_dicts = [m.state_dict() for m in models]\n",
    "    # print(models[0])\n",
    "    # print(models[0].get_conv_layers())\n",
    "    # print(models[1].state_dict().keys())\n",
    "    # shared_keys = get_shared_keys(models)\n",
    "    # print(shared_keys)\n",
    "    # print(conv_layers_list)\n",
    "   \n",
    "    max_layers = max(num_layers_list)\n",
    "\n",
    "    aggregatable_layers_per_model = []\n",
    "#     for i, model in enumerate(models):\n",
    "#         current_layers = num_layers_list[i]\n",
    "#         deeper_models = [models[j] for j in range(len(models)) if num_layers_list[j] >= current_layers]\n",
    "#         possible_counts = [compare_models_aggregatable_layers(model, m_deep) for m_deep in deeper_models]\n",
    "#         aggregatable = min(possible_counts) if possible_counts else 0\n",
    "#         # if current_layers == max_layers:\n",
    "#         #     \n",
    "#         #     aggregatable = current_layers\n",
    "#         # else:\n",
    "#         #     \n",
    "#         #     deeper_models = [models[j] for j in range(len(models)) if num_layers_list[j] > current_layers]\n",
    "#         #     possible_counts = [compare_models_aggregatable_layers(model, m_deep) for m_deep in deeper_models]\n",
    "#         #     aggregatable = min(possible_counts) if possible_counts else 0\n",
    "#         aggregatable_layers_per_model.append(aggregatable)\n",
    "\n",
    "#    \n",
    "#     print(\"✅ Aggregatable convolutional layers per model:\")\n",
    "#     for i, num in enumerate(num_layers_list):\n",
    "#         print(f\"Model {i+1} ({num} conv layers): {aggregatable_layers_per_model[i]} aggregatable layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4723c1b-ee4e-4d3f-afaf-16087dbf8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_aggregation(w, lens=None, shared_keys=None):\n",
    "    \n",
    "    # 1. 分类客户端模型（按层数）并排序\n",
    "    layer_groups = defaultdict(list)\n",
    "    # for i, model in enumerate(w):\n",
    "    #     layer_groups[len(model.state_dict().keys())].append(i)\n",
    "    # layer_types = sorted(layer_groups.keys())  \n",
    "    for i, model in enumerate(models):\n",
    "        num_conv_params = 2 * len(model.get_conv_layers())  \n",
    "        layer_groups[num_conv_params].append(i)\n",
    "    layer_types = sorted(layer_groups.keys()) \n",
    "    \n",
    "    \n",
    "    # 2. 初始化全局模型（选择最大层数的第一个客户端作为模板）\n",
    "    max_layer = max(layer_types)\n",
    "    w_avg = copy.deepcopy(next(m for m in w if len(m.state_dict().keys()) == max_layer))\n",
    "    \n",
    "    # 3. 构建分层权重映射表\n",
    "    weight_groups = {}\n",
    "    for depth in range(1, max_layer + 1):\n",
    "        # 筛选拥有至少depth层的客户端\n",
    "        valid_clients = [i for i, model in enumerate(w) if len(model.keys()) >= depth]\n",
    "        \n",
    "        if lens is None:\n",
    "            # 均分权重\n",
    "            weight_groups[depth] = [1.0/len(valid_clients)] * len(valid_clients)\n",
    "        else:\n",
    "            # 按样本量加权\n",
    "            total = sum(lens[i] for i in valid_clients)\n",
    "            weight_groups[depth] = [lens[i]/total for i in valid_clients]\n",
    "    \n",
    "    # 4. 按层深度聚合\n",
    "    layer_names = sorted(w_avg.keys())  # 确保层顺序一致\n",
    "    for depth, key in enumerate(layer_names, 1):\n",
    "        if shared_keys is not None and key not in shared_keys:\n",
    "            continue\n",
    "        \n",
    "        # 获取当前层有效的客户端和权重\n",
    "        valid_indices = [i for i, model in enumerate(w) if key in model]\n",
    "        weights = [weight_groups[depth][valid_indices.index(i)] for i in valid_indices]\n",
    "        \n",
    "        # 加权聚合\n",
    "        w_avg[key] = torch.zeros_like(w[valid_indices[0]][key])\n",
    "        for idx, weight in zip(valid_indices, weights):\n",
    "            w_avg[key] += w[idx][key] * weight\n",
    "    \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a491f9b-11f9-4440-8664-bd9c6626441d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
